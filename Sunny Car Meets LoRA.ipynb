{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yHNbl3O_NSS0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Team the REPLY list\n",
        "# Adapted from various open source notebooks\n",
        "# Licensed under Apache-2.0"
      ],
      "metadata": {
        "id": "PgT3fslWuJ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36584f2-64d5-4e9c-8086-0342206acbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 12 17:46:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 1604, done.\u001b[K\n",
            "remote: Counting objects: 100% (303/303), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 1604 (delta 195), reused 220 (delta 140), pack-reused 1301\u001b[K\n",
            "Receiving objects: 100% (1604/1604), 3.26 MiB | 2.58 MiB/s, done.\n",
            "Resolving deltas: 100% (1005/1005), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m207.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "#root_dir\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir,\"deps\")\n",
        "repo_dir = os.path.join(root_dir,\"kohya-trainer\")\n",
        "training_dir = os.path.join(root_dir,\"LoRA\")\n",
        "pretrained_model = os.path.join(root_dir,\"pretrained_model\")\n",
        "controlnet_dir = os.path.join(root_dir,\"controlnet\")\n",
        "vae_dir = os.path.join(root_dir,\"vae\")\n",
        "config_dir = os.path.join(training_dir,\"config\")\n",
        "\n",
        "#repo_dir\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "tools_dir = os.path.join(repo_dir,\"tools\")\n",
        "finetune_dir = os.path.join(repo_dir,\"finetune\")\n",
        "\n",
        "for store in [\"root_dir\", \"deps_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"controlnet_dir\", \"vae_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "  with capture.capture_output() as cap:\n",
        "    %store {store}\n",
        "    del cap\n",
        "\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "branch = \"\" \n",
        "install_xformers = True\n",
        "\n",
        "for dir in [deps_dir, training_dir, config_dir, pretrained_model, vae_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "  \n",
        "def clone_repo(url):\n",
        "  if not os.path.exists(repo_dir):\n",
        "    os.chdir(root_dir)\n",
        "    !git clone {url} {repo_dir}\n",
        "  else:\n",
        "    os.chdir(repo_dir)\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "\n",
        "clone_repo(repo_url)\n",
        "\n",
        "if branch:\n",
        "  os.chdir(repo_dir)\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, 'r') as deps:\n",
        "      deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "\n",
        "def install_dependencies():\n",
        "  !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "  if install_xformers:\n",
        "    !pip install -q --pre xformers\n",
        "    !pip install -q --pre triton\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config):\n",
        "    write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "install_dependencies()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## File Explorer\n",
        "import threading\n",
        "from google.colab import output\n",
        "from imjoy_elfinder.app import main\n",
        "%store -r\n",
        "\n",
        "thread = threading.Thread(target=main, args=[[\"--root-dir=/content\", \"--port=8766\"]])\n",
        "thread.start()\n",
        "\n",
        "open_in_new_tab = True #@param {type:\"boolean\"}\n",
        "\n",
        "if open_in_new_tab:\n",
        "  output.serve_kernel_port_as_window(8766)\n",
        "else:\n",
        "  output.serve_kernel_port_as_iframe(8765, height='500')\n"
      ],
      "metadata": {
        "id": "ZmIRAxgEQESm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "127e3b0f-0905-4373-9183-1c230c0fa9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8766, \"/\", \"https://localhost:8766/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========ImJoy elFinder server is running=========\n",
            "http://127.0.0.1:8766\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Download Model\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "def install(checkpoint_name, url, ext, path):\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {path} -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "install(\"Anything-v4-5\", \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \"ckpt\", pretrained_model)\n",
        "install(\"control_sd15_depth\", \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth\", \"ckpt\", controlnet_dir)\n",
        "!wget -O {controlnet_dir}/control_sd15_depth.yaml https://huggingface.co/webui/ControlNet-modules-safetensors/raw/main/cldm_v15.yaml\n",
        "install(\"stablediffusion.vae\", \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\", \"pt\", vae_dir)"
      ],
      "metadata": {
        "id": "wmnsZwClN1XL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ff2f02-a905-4773-b97f-e73baf1a3570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b584f5|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/pretrained_model/Anything-v4-5.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            " *** Download Progress Summary as of Sun Mar 12 17:17:35 2023 *** \n",
            "=\n",
            "[#a02217 2.5GiB/5.3GiB(47%) CN:16 DL:240MiB ETA:11s]\n",
            "FILE: /content/controlnet/control_sd15_depth.ckpt\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sun Mar 12 17:17:46 2023 *** \n",
            "=\n",
            "[#a02217 4.0GiB/5.3GiB(76%) CN:16 DL:145MiB ETA:8s]\n",
            "FILE: /content/controlnet/control_sd15_depth.ckpt\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "a02217|\u001b[1;32mOK\u001b[0m  |   200MiB/s|/content/controlnet/control_sd15_depth.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "--2023-03-12 17:17:52--  https://huggingface.co/webui/ControlNet-modules-safetensors/raw/main/cldm_v15.yaml\n",
            "Resolving huggingface.co (huggingface.co)... 35.168.63.213, 52.206.41.232, 44.197.34.50, ...\n",
            "Connecting to huggingface.co (huggingface.co)|35.168.63.213|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1949 (1.9K) [text/plain]\n",
            "Saving to: ‘/content/controlnet/control_sd15_depth.yaml’\n",
            "\n",
            "/content/controlnet 100%[===================>]   1.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-12 17:17:52 (391 MB/s) - ‘/content/controlnet/control_sd15_depth.yaml’ saved [1949/1949]\n",
            "\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "4c6b50|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/vae/stablediffusion.vae.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Locating Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input. Regularization Images is `optional` and can be skipped.\n",
        "import os\n",
        "from IPython.utils import capture\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/LoRA/train_data\" #@param {type:'string'}\n",
        "reg_data_dir = \"/content/LoRA/reg_data\" #@param {type:'string'}\n",
        "\n",
        "for image_dir in [train_data_dir, reg_data_dir]:\n",
        "  if image_dir:\n",
        "    with capture.capture_output() as cap:\n",
        "      os.makedirs(image_dir, exist_ok=True)\n",
        "      %store image_dir\n",
        "      del cap\n",
        "\n",
        "print(f\"Your train data directory : {train_data_dir}\")\n",
        "if reg_data_dir:\n",
        "  print(f\"Your reg data directory : {reg_data_dir}\")\n"
      ],
      "metadata": {
        "id": "kh7CeDqK4l3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b0d2e6-36bc-4564-8589-536c7d2d7f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your train data directory : /content/LoRA/train_data\n",
            "Your reg data directory : /content/LoRA/reg_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Unzip Dataset\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "#@markdown Specify this section if your dataset is in a `zip` file and has been uploaded somewhere. This will download your dataset and automatically extract it to the `train_data_dir` if the `unzip_to` is empty. \n",
        "zipfile_url = \"https://drive.google.com/file/d/1uXySudpfqDQEZ0JUl4Ggzp3zFCpUASuK/view?usp=sharing\" #@param {'type': 'string'}\n",
        "zipfile_name = \"Archive.zip\"\n",
        "unzip_to = \"/content/LoRA/train_data\" #@param {'type': 'string'}\n",
        "\n",
        "hf_token = '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'\n",
        "user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "\n",
        "if unzip_to:\n",
        "  os.makedirs(unzip_to, exist_ok=True)\n",
        "else:\n",
        "  unzip_to = train_data_dir\n",
        "\n",
        "def download_dataset(url):\n",
        "  if url.startswith(\"/content\"):\n",
        "    !unzip -j -o {url} -d \"{train_data_dir}\"\n",
        "  elif url.startswith(\"https://drive.google.com\"):\n",
        "    os.chdir(root_dir)\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {zipfile_name} {url}\n",
        "\n",
        "download_dataset(zipfile_url)\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "if not zipfile_url.startswith(\"/content\"):\n",
        "  !unzip -j -o \"{root_dir}/{zipfile_name}\" -d \"{unzip_to}\"\n",
        "  os.remove(f\"{root_dir}/{zipfile_name}\")\n",
        "\n",
        "files_to_move = (\"meta_cap.json\", \\\n",
        "                 \"meta_cap_dd.json\", \\\n",
        "                 \"meta_lat.json\", \\\n",
        "                 \"meta_clean.json\")\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "  file_path = os.path.join(train_data_dir, filename)\n",
        "  if filename in files_to_move:\n",
        "    if not os.path.exists(file_path):\n",
        "      shutil.move(file_path, training_dir)\n",
        "    else: \n",
        "      os.remove(file_path)"
      ],
      "metadata": {
        "id": "eFFHVTWNZGbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9019b4c-e836-436d-bd4c-86a6daca9378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uXySudpfqDQEZ0JUl4Ggzp3zFCpUASuK\n",
            "To: /content/Archive.zip\n",
            "\r  0% 0.00/10.5M [00:00<?, ?B/s]\r100% 10.5M/10.5M [00:00<00:00, 173MB/s]\n",
            "Archive:  /content/Archive.zip\n",
            "  inflating: /content/LoRA/train_data/DALLE_2023-03-12_10.39.46_-_beautiful_car_parked_on_cobble_stone_road_in_sunny_sky.png  \n",
            "  inflating: /content/LoRA/train_data/._DALLE_2023-03-12_10.39.46_-_beautiful_car_parked_on_cobble_stone_road_in_sunny_sky.png  \n",
            "  inflating: /content/LoRA/train_data/DALLE_2023-03-12_10.41.13_-_a_beautiful_whole_car_parked_on_cobble_road_in_sunny_sky.png  \n",
            "  inflating: /content/LoRA/train_data/._DALLE_2023-03-12_10.41.13_-_a_beautiful_whole_car_parked_on_cobble_road_in_sunny_sky.png  \n",
            "  inflating: /content/LoRA/train_data/DALLE_2023-03-12_11.10.08_-_ferrari_on_cobble_road_in_sunny_day.png  \n",
            "  inflating: /content/LoRA/train_data/._DALLE_2023-03-12_11.10.08_-_ferrari_on_cobble_road_in_sunny_day.png  \n",
            "  inflating: /content/LoRA/train_data/DALLE_2023-03-12_11.16.56_-_ferrari_on_cobble_road_in_sunny_day_realistic_top_view.png  \n",
            "  inflating: /content/LoRA/train_data/._DALLE_2023-03-12_11.16.56_-_ferrari_on_cobble_road_in_sunny_day_realistic_top_view.png  \n",
            "  inflating: /content/LoRA/train_data/DALLE_2023-03-12_11.20.58_-_ferrari_on_cobble_road_in_sunny_day_beautiful.png  \n",
            "  inflating: /content/LoRA/train_data/._DALLE_2023-03-12_11.20.58_-_ferrari_on_cobble_road_in_sunny_day_beautiful.png  \n",
            "  inflating: /content/LoRA/train_data/FRYThmJXMAge8aF.jpg  \n",
            "  inflating: /content/LoRA/train_data/._FRYThmJXMAge8aF.jpg  \n",
            "  inflating: /content/LoRA/train_data/mirz8s1idgx41.webp  \n",
            "  inflating: /content/LoRA/train_data/._mirz8s1idgx41.webp  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4.2.1. BLIP Captioning\n",
        "#@markdown [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) is a pre-training framework for unified vision-language understanding and generation, which achieves state-of-the-art results on a wide range of vision-language tasks.\n",
        "#@markdown In short, it can be used as a tool for image captioning. Example: `astronaut riding a horse in space`. \n",
        "import os\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "\n",
        "batch_size = 8 #@param {type:'number'}\n",
        "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "beam_search = True #@param {type:'boolean'}\n",
        "min_length = 5 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
        "max_length = 75 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
        "\n",
        "!python make_captions.py \\\n",
        "  \"{train_data_dir}\" \\\n",
        "  --batch_size {batch_size} \\\n",
        "  {\"--beam_search\" if beam_search else \"\"} \\\n",
        "  --min_length {min_length} \\\n",
        "  --max_length {max_length} \\\n",
        "  --caption_extension .caption \\\n",
        "  --max_data_loader_n_workers {max_data_loader_n_workers}"
      ],
      "metadata": {
        "id": "xvGx2Ikhc8iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2113ff-d72d-47c1-d500-85e7a196bf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-12 16:57:47.615724: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-12 16:57:49.158671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 16:57:49.158848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 16:57:49.158875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "load images from /content/LoRA/train_data\n",
            "found 7 images.\n",
            "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 1.24MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 3.41kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 206kB/s]\n",
            "100% 1.66G/1.66G [00:07<00:00, 239MB/s]\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "BLIP loaded\n",
            "100% 1/1 [00:00<00:00,  1.14it/s]\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Add Custom Caption/Tag\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "#@markdown Add custom tags here.\n",
        "extension = \".caption\" #@param [\".txt\", \".caption\"]\n",
        "custom_tag = \"mytag\" #@param {type:\"string\"}\n",
        "#@markdown Enable to append custom tags at the end of lines.\n",
        "append = True #@param {type:\"boolean\"}\n",
        "\n",
        "def add_tag(filename, tag, append):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "        \t    \t\n",
        "    tag = \", \".join(tag.split())\n",
        "    tag = tag.replace(\"_\", \" \")\n",
        "    \n",
        "    if tag in contents:\n",
        "        return\n",
        "\n",
        "    if append:\n",
        "      contents = contents.rstrip() + \", \" + tag\n",
        "    else:\n",
        "      contents = tag + \", \" + contents\n",
        "    \n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "if not any([filename.endswith(\".\" + extension) for filename in os.listdir(train_data_dir)]):\n",
        "    for filename in os.listdir(train_data_dir):\n",
        "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
        "            open(os.path.join(train_data_dir, filename.split(\".\")[0] + extension), \"w\").close()\n",
        "\n",
        "tags = custom_tag.split()\n",
        "\n",
        "if custom_tag:\n",
        "  for filename in os.listdir(train_data_dir):\n",
        "      if filename.endswith(extension):\n",
        "          for tag in tags:\n",
        "              add_tag(os.path.join(train_data_dir, filename), tag, append)\n"
      ],
      "metadata": {
        "id": "_mLVURhM9PFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Training Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.1. Model Config\n",
        "from google.colab import drive\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = True #@param {type:\"boolean\"}\n",
        "project_name = \"REPLY\" #@param {type:\"string\"}\n",
        "if not project_name:\n",
        "  project_name = \"last\"\n",
        "pretrained_model_name_or_path = \"/content/pretrained_model/Anything-v4-5.ckpt\" #@param {type:\"string\"}\n",
        "vae = \"/content/vae/stablediffusion.vae.pt\"  #@param {type:\"string\"}\n",
        "output_dir = \"/content/LoRA/output\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/LoRA/output` by default\n",
        "output_to_drive = False #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/LoRA/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "sample_dir = os.path.join(output_dir, \"sample\")\n",
        "for dir in [output_dir, sample_dir]:\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "print(\"Project Name: \", project_name)\n",
        "print(\"Model Version: Stable Diffusion V1.x\") if not v2 else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x\") if v2 and not v_parameterization else \"\"\n",
        "print(\"Model Version: Stable Diffusion V2.x 768v\") if v2 and v_parameterization else \"\"\n",
        "print(\"Pretrained Model Path: \", pretrained_model_name_or_path) if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
        "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
        "print(\"Output Path: \", output_dir)"
      ],
      "metadata": {
        "id": "H_Q23fUEJhnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a6ce4a-9500-47a7-dfb1-2e127e9b1eb0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Name:  REPLY\n",
            "Model Version: Stable Diffusion V1.x\n",
            "Pretrained Model Path:  /content/pretrained_model/Anything-v4-5.ckpt\n",
            "VAE Path:  /content/vae/stablediffusion.vae.pt\n",
            "Output Path:  /content/LoRA/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.2. Dataset Config\n",
        "import toml\n",
        "\n",
        "#@markdown ### Dreambooth Config\n",
        "train_repeats = 10 #@param {type:\"number\"}\n",
        "reg_repeats = 1 #@param {type:\"number\"}\n",
        "instance_token = \"mksks\" #@param {type:\"string\"}\t\n",
        "class_token = \" style\" #@param {type:\"string\"}\t \n",
        "#@markdown ### <br>General Config\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "flip_aug = False #@param {type:\"boolean\"}\t\n",
        "caption_extension = \".txt\" #@param [\"none\", \".txt\", \".caption\"]\t\n",
        "caption_dropout_rate = 0 #@param {type:\"slider\", min:0, max:1, step:0.05}\t\n",
        "caption_dropout_every_n_epochs = 0 #@param {type:\"number\"}\n",
        "keep_tokens = 0 #@param {type:\"number\"}\n",
        "\n",
        "config = {\n",
        "    \"general\": {\n",
        "        \"enable_bucket\": True,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"shuffle_caption\": True,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"resolution\": resolution,\n",
        "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,           \n",
        "            \"caption_dropout_rate\": caption_dropout_rate if caption_extension == \".caption\" else 0,\n",
        "            \"caption_tag_dropout_rate\": caption_dropout_rate if caption_extension == \".txt\" else 0,\n",
        "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs,\n",
        "            \"flip_aug\": flip_aug,\n",
        "            \"color_aug\": False,\n",
        "            \"face_crop_aug_range\": None,\n",
        "            \"subsets\": [\n",
        "                {\n",
        "                    \"image_dir\": train_data_dir,\n",
        "                    \"class_tokens\": f\"{instance_token} {class_token}\",\n",
        "                    \"num_repeats\": train_repeats,\n",
        "                },\n",
        "                {\n",
        "                    \"is_reg\": True,\n",
        "                    \"image_dir\": reg_data_dir,\n",
        "                    \"class_tokens\": class_token,\n",
        "                    \"num_repeats\": reg_repeats,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(dataset_config, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G5u_DhFeyJ6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cfee87-5f73-4af1-84af-963b2347aa4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[datasets]]\n",
            "resolution = 512\n",
            "min_bucket_reso = 256\n",
            "max_bucket_reso = 1024\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "flip_aug = false\n",
            "color_aug = false\n",
            "[[datasets.subsets]]\n",
            "image_dir = \"/content/LoRA/train_data\"\n",
            "class_tokens = \"mksks  style\"\n",
            "num_repeats = 10\n",
            "\n",
            "[[datasets.subsets]]\n",
            "is_reg = true\n",
            "image_dir = \"/content/LoRA/reg_data\"\n",
            "class_tokens = \" style\"\n",
            "num_repeats = 1\n",
            "\n",
            "\n",
            "[general]\n",
            "enable_bucket = true\n",
            "caption_extension = \".txt\"\n",
            "shuffle_caption = true\n",
            "keep_tokens = 0\n",
            "bucket_reso_steps = 64\n",
            "bucket_no_upscale = false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.3. Sample Prompt Config\n",
        "enable_sample = True #@param {type:\"boolean\"}\n",
        "sample_every_n_type = \"sample_every_n_epochs\" #@param [\"sample_every_n_steps\", \"sample_every_n_epochs\"]\n",
        "sample_every_n_type_value = 1 #@param {type:\"number\"}\n",
        "if not enable_sample:\n",
        "  sample_every_n_type_value = 999999\n",
        "sampler = \"dpmsolver++\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "prompt = \"masterpiece, best quality, cobblestone, cobbled road, car, mytag\" #@param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"\n",
        "width = \"512\" #@param {type:\"string\"}\n",
        "height = \"512\" #@param {type:\"string\"}\n",
        "scale = 7 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "steps = 28 #@param {type:\"number\"}\n",
        "\n",
        "sample_str = f\"\"\"\n",
        "  {prompt} \\\n",
        "  --n {negative} \\\n",
        "  --w {width} \\\n",
        "  --h {height} \\\n",
        "  --l {scale} \\\n",
        "  --s {steps} \\\n",
        "  {f\"--d \" + seed if seed > 0 else \"\"} \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
        "\n",
        "with open(prompt_path, \"w\") as f:\n",
        "    f.write(sample_str)\n",
        "\n"
      ],
      "metadata": {
        "id": "QCEAEGhDG1QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.4. LoRA and Optimizer Config\n",
        "\n",
        "#@markdown ### LoRA Config:\n",
        "#@markdown - `networks.lora` is normal and default [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts) LoRA.\n",
        "#@markdown - `lycoris.kohya` is a python package for LoRA module. Previously LoCon. Currently there are 2 LoRA algorithms: LoCon and LoRA with [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) representation. Put `algo=lora` for LoCon or `algo=loha` for Hadamard Product in `network_args`. Read: [KohakuBlueleaf/LyCORIS](https://github.com/KohakuBlueleaf/Lycoris).\n",
        "#@markdown - `locon.locon_kohya` <font color = 'red'> (backward compatibility, deprecated)</font> is LoRA for convolutional network. In short, it's the same LoRA but training almost all layers including normal LoRA layer. Read: [KohakuBlueleaf/LoCon](https://github.com/KohakuBlueleaf/LoCon).\n",
        "network_module = \"lycoris.kohya\" #@param [\"networks.lora\", \"lycoris.kohya\", \"locon.locon_kohya\"]\n",
        "\n",
        "#@markdown For custom `networks_module` you need to set additional `network_args`, e.g.: `[\"conv_dim=32\",\"conv_alpha=16\"]`\n",
        "network_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Some LoRA guides using 128 dim/alpha, but it's recommended to not specify `network_dim` and `alpha` higher than `48-64`. \n",
        "#@markdown The smaller `network_dim` is, the smaller the model size is. The larger `network_alpha` is, the closer the model is to a fully fine-tuned model. Read: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "network_dim = 32 #@param {'type':'number'}\n",
        "network_alpha = 16 #@param {'type':'number'}\n",
        "#@markdown You can specify this field for resume training.\n",
        "network_weight = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown ### <br>Optimizer Config:\n",
        "#@markdown `AdamW8bit` was the old `--use_8bit_adam`.\n",
        "optimizer_type = \"AdamW8bit\" #@param [\"AdamW\", \"AdamW8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation\", \"AdaFactor\"]\n",
        "#@markdown Additional arguments for optimizer, e.g: `[\"decouple=true\",\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"\" #@param {'type':'string'}\n",
        "#@markdown Set `unet_lr` to `1.0` if you use `DAdaptation` optimizer, because it's a [free learning rate](https://github.com/facebookresearch/dadaptation) algorithm. \n",
        "#@markdown However `text_encoder_lr = 1/2 * unet_lr` still applied, so you need to set `0.5` for `text_encoder_lr`.\n",
        "#@markdown Also actually you don't need to specify `learning_rate` value if both `unet_lr` and `text_encoder_lr` are defined.\n",
        "train_unet = True #@param {'type':'boolean'}\n",
        "unet_lr = 1e-4 #@param {'type':'number'}\n",
        "train_text_encoder = True #@param {'type':'boolean'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = 0 #@param {'type':'number'}\n",
        "#@markdown You can define `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial` in the field below.\n",
        "lr_scheduler_num_cycles = 0 #@param {'type':'number'}\n",
        "lr_scheduler_power = 0 #@param {'type':'number'}\n",
        "\n",
        "print(\"- LoRA Config:\")\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(\"network args:\", network_args)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if not network_weight:\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weight):\n",
        "    print(\"Loading LoRA weight:\", network_weight)\n",
        "  else:\n",
        "    print(f\"{network_weight} does not exist.\")\n",
        "    network_weight = \"\"\n",
        "\n",
        "print(\"- Optimizer Config:\")\n",
        "print(f\"Using {optimizer_type} as Optimizer\")\n",
        "if optimizer_args:\n",
        "  print(f\"Optimizer Args :\", optimizer_args)\n",
        "if train_unet and train_text_encoder:\n",
        "  print(f\"Train UNet and Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "if train_unet and not train_text_encoder:\n",
        "  print(f\"Train UNet only\")\n",
        "  print(\"UNet learning rate: \", unet_lr)\n",
        "if train_text_encoder and not train_unet:\n",
        "  print(f\"Train Text Encoder only\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr)\n",
        "print(\"Learning rate warmup steps: \", lr_warmup_steps)\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- lr_scheduler_num_cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- lr_scheduler_power: \", lr_scheduler_power)\n",
        "\n"
      ],
      "metadata": {
        "id": "iD5Ecamp4rVW",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda14376-8c07-48b4-ca70-745c0064381d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- LoRA Config:\n",
            "Loading network module: lycoris.kohya\n",
            "network args: \n",
            "lycoris.kohya dim set to: 32\n",
            "lycoris.kohya alpha set to: 16\n",
            "No LoRA weight loaded.\n",
            "- Optimizer Config:\n",
            "Using AdamW8bit as Optimizer\n",
            "Train UNet and Text Encoder\n",
            "UNet learning rate:  0.0001\n",
            "Text encoder learning rate:  5e-05\n",
            "Learning rate warmup steps:  0\n",
            "Learning rate Scheduler: constant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.5. Training Config\n",
        "\n",
        "import toml\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "lowram = True #@param {type:\"boolean\"}\n",
        "noise_offset = 0.0 #@param {type:\"number\"}\n",
        "num_epochs = 10 #@param {type:\"number\"}\n",
        "train_batch_size = 2 #@param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_every_n_epochs\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/LoRA/logs\"\n",
        "prior_loss_weight = 1.0\n",
        "              \n",
        "os.chdir(repo_dir)\n",
        "\n",
        "config = {\n",
        "    \"model_arguments\": {\n",
        "        \"v2\": v2,\n",
        "        \"v_parameterization\": v_parameterization if v2 and v_parameterization else False,\n",
        "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "        \"vae\": vae,\n",
        "    },\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\": False,\n",
        "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
        "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
        "        \"network_weights\": network_weight,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_args\": eval(network_args) if network_args else None,\n",
        "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
        "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
        "        \"training_comment\": None,\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\": optimizer_type,\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "        \"debug_dataset\": False,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\": output_dir,\n",
        "        \"output_name\": project_name,\n",
        "        \"save_precision\": save_precision,\n",
        "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
        "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
        "        \"save_last_n_epochs\": None,\n",
        "        \"save_state\": None,\n",
        "        \"save_last_n_epochs_state\": None,\n",
        "        \"resume\": None,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"max_token_length\": 225,\n",
        "        \"mem_eff_attn\": False,\n",
        "        \"xformers\": True,\n",
        "        \"max_train_epochs\": num_epochs,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\": seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\": gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"mixed_precision\": mixed_precision,\n",
        "        \"clip_skip\": clip_skip if not v2 else None,\n",
        "        \"logging_dir\": logging_dir,\n",
        "        \"log_prefix\": project_name,\n",
        "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
        "        \"lowram\": lowram,\n",
        "    },\n",
        "    \"sample_prompt_arguments\":{\n",
        "        \"sample_every_n_steps\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_steps\" else None,\n",
        "        \"sample_every_n_epochs\": sample_every_n_type_value if sample_every_n_type == \"sample_every_n_epochs\" else None,\n",
        "        \"sample_sampler\": sampler,\n",
        "    },\n",
        "    \"dreambooth_arguments\":{\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"saving_arguments\":{\n",
        "        \"save_model_as\": save_model_as\n",
        "    },\n",
        "}\n",
        "\n",
        "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
        "\n",
        "for key in config:\n",
        "    if isinstance(config[key], dict):\n",
        "        for sub_key in config[key]:\n",
        "            if config[key][sub_key] == \"\":\n",
        "                config[key][sub_key] = None\n",
        "    elif config[key] == \"\":\n",
        "        config[key] = None\n",
        "\n",
        "config_str = toml.dumps(config)\n",
        "\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(config_str)\n",
        "\n",
        "print(config_str)"
      ],
      "metadata": {
        "id": "-Z4w3lfFKLjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ac26fb-6b4e-423c-a06d-c3851fc49df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[model_arguments]\n",
            "v2 = false\n",
            "v_parameterization = false\n",
            "pretrained_model_name_or_path = \"/content/pretrained_model/Anything-v4-5.ckpt\"\n",
            "vae = \"/content/vae/stablediffusion.vae.pt\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "unet_lr = 0.0001\n",
            "text_encoder_lr = 5e-5\n",
            "network_module = \"lycoris.kohya\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_train_unet_only = false\n",
            "network_train_text_encoder_only = false\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdamW8bit\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 1.0\n",
            "lr_scheduler = \"constant\"\n",
            "lr_warmup_steps = 0\n",
            "\n",
            "[dataset_arguments]\n",
            "cache_latents = true\n",
            "debug_dataset = false\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/content/LoRA/output\"\n",
            "output_name = \"REPLY\"\n",
            "save_precision = \"fp16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 2\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "xformers = true\n",
            "max_train_epochs = 10\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = false\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"fp16\"\n",
            "clip_skip = 2\n",
            "logging_dir = \"/content/LoRA/logs\"\n",
            "log_prefix = \"REPLY\"\n",
            "lowram = true\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"dpmsolver++\"\n",
            "\n",
            "[dreambooth_arguments]\n",
            "prior_loss_weight = 1.0\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.6. Start Training\n",
        "\n",
        "#@markdown Check your config here if you want to edit something: \n",
        "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.txt\n",
        "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
        "#@markdown - `dataset_config` : /content/LoRA/config/dataset_config.toml\n",
        "\n",
        "#@markdown Generated sample can be seen here: /content/LoRA/output/sample\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "sample_prompt = \"/content/LoRA/config/sample_prompt.txt\" #@param {type:'string'}\n",
        "config_file = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "dataset_config = \"/content/LoRA/config/dataset_config.toml\" #@param {type:'string'}\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!accelerate launch \\\n",
        "  --config_file={accelerate_config} \\\n",
        "  --num_cpu_threads_per_process=1 \\\n",
        "  train_network.py \\\n",
        "  --sample_prompts={sample_prompt} \\\n",
        "  --dataset_config={dataset_config} \\\n",
        "  --config_file={config_file}\n",
        "\n"
      ],
      "metadata": {
        "id": "p_SHtbFwHVl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2a5033-d584-4a4f-d3ae-fb6642a79167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-12 17:08:58.695831: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-12 17:09:00.040106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 17:09:00.040239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 17:09:00.040269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-12 17:09:05.310764: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-12 17:09:06.404730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 17:09:06.404899: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-12 17:09:06.404928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Loading settings from /content/LoRA/config/config_file.toml...\n",
            "/content/LoRA/config/config_file\n",
            "prepare tokenizer\n",
            "update token length: 225\n",
            "Load dataset config from /content/LoRA/config/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/LoRA/train_data contains 7 image files\n",
            "found directory /content/LoRA/reg_data contains 0 image files\n",
            "ignore subset with image_dir='/content/LoRA/reg_data': no images found / 画像が見つからないためサブセットを無視します\n",
            "70 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 2\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/LoRA/train_data\"\n",
            "    image_count: 7\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    is_reg: False\n",
            "    class_tokens: mksks  style\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 7/7 [00:00<00:00, 96.79it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (512, 512), count: 60\n",
            "bucket 1: resolution (640, 384), count: 10\n",
            "mean ar error (without repeats): 0.03240194149285057\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (…)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 601kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 1.71G/1.71G [00:07<00:00, 224MB/s]\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /content/vae/stablediffusion.vae.pt\n",
            "additional VAE loaded\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100% 7/7 [00:07<00:00,  1.08s/it]\n",
            "import network module: lycoris.kohya\n",
            "Using rank adaptation algo: lora\n",
            "Use Dropout value: 0.0\n",
            "Create LoCon Module\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "Create LoCon Module\n",
            "create LoRA for U-Net: 278 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 350\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 70\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 35\n",
            "  num epochs / epoch数: 10\n",
            "  batch size per device / バッチサイズ: 2\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 350\n",
            "steps:   0% 0/350 [00:00<?, ?it/s]epoch 1/10\n",
            "steps:  10% 35/350 [00:26<03:57,  1.32it/s, loss=0.192]saving checkpoint: /content/LoRA/output/REPLY-000001.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 35\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  5.06it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.97it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.94it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:04,  4.93it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:04,  4.90it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.91it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.92it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.92it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:03,  4.92it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.91it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.92it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.93it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.93it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  4.93it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.92it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.92it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.92it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.92it/s]\u001b[A\n",
            " 68% 19/28 [00:03<00:01,  4.92it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.92it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.91it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.90it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:01,  4.89it/s]\u001b[A\n",
            " 86% 24/28 [00:04<00:00,  4.88it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.88it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.87it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.89it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  4.91it/s]\n",
            "epoch 2/10\n",
            "steps:  20% 70/350 [00:57<03:50,  1.21it/s, loss=0.156]saving checkpoint: /content/LoRA/output/REPLY-000002.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 70\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:09,  2.74it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:09,  2.86it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:07,  3.49it/s]\u001b[A\n",
            " 14% 4/28 [00:01<00:06,  3.91it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  4.17it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:05,  4.33it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.45it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.55it/s]\u001b[A\n",
            " 32% 9/28 [00:02<00:04,  4.62it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.66it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.67it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.69it/s]\u001b[A\n",
            " 46% 13/28 [00:03<00:03,  4.71it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:02,  4.72it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.74it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.75it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.73it/s]\u001b[A\n",
            " 64% 18/28 [00:04<00:02,  4.74it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.75it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.75it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.75it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.74it/s]\u001b[A\n",
            " 82% 23/28 [00:05<00:01,  4.75it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.75it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.75it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.75it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.74it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.53it/s]\n",
            "epoch 3/10\n",
            "steps:  30% 105/350 [01:30<03:31,  1.16it/s, loss=0.154]saving checkpoint: /content/LoRA/output/REPLY-000003.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 105\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.83it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.75it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.72it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.66it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:04,  4.63it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.64it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.65it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.65it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.63it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.64it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.65it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.66it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.66it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.65it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.65it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.37it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  3.75it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  3.97it/s]\u001b[A\n",
            " 82% 23/28 [00:05<00:01,  3.35it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:01,  2.88it/s]\u001b[A\n",
            " 89% 25/28 [00:06<00:01,  2.53it/s]\u001b[A\n",
            " 93% 26/28 [00:07<00:01,  1.97it/s]\u001b[A\n",
            " 96% 27/28 [00:07<00:00,  1.98it/s]\u001b[A\n",
            "100% 28/28 [00:08<00:00,  3.38it/s]\n",
            "epoch 4/10\n",
            "steps:  40% 140/350 [02:08<03:13,  1.09it/s, loss=0.157]saving checkpoint: /content/LoRA/output/REPLY-000004.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 140\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.70it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:06,  3.83it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:06,  3.61it/s]\u001b[A\n",
            " 14% 4/28 [00:01<00:06,  3.71it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  3.96it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:05,  3.98it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:05,  3.58it/s]\u001b[A\n",
            " 29% 8/28 [00:02<00:06,  3.01it/s]\u001b[A\n",
            " 32% 9/28 [00:02<00:05,  3.34it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:04,  3.65it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:04,  3.91it/s]\u001b[A\n",
            " 43% 12/28 [00:03<00:03,  4.02it/s]\u001b[A\n",
            " 46% 13/28 [00:03<00:03,  4.20it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.33it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.42it/s]\u001b[A\n",
            " 57% 16/28 [00:04<00:02,  4.48it/s]\u001b[A\n",
            " 61% 17/28 [00:04<00:02,  4.51it/s]\u001b[A\n",
            " 64% 18/28 [00:04<00:02,  4.55it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.58it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.49it/s]\u001b[A\n",
            " 75% 21/28 [00:05<00:01,  4.54it/s]\u001b[A\n",
            " 79% 22/28 [00:05<00:01,  4.57it/s]\u001b[A\n",
            " 82% 23/28 [00:05<00:01,  4.57it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.59it/s]\u001b[A\n",
            " 89% 25/28 [00:06<00:00,  4.54it/s]\u001b[A\n",
            " 93% 26/28 [00:06<00:00,  4.57it/s]\u001b[A\n",
            " 96% 27/28 [00:06<00:00,  4.58it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.19it/s]\n",
            "epoch 5/10\n",
            "steps:  50% 175/350 [02:42<02:42,  1.08it/s, loss=0.13] saving checkpoint: /content/LoRA/output/REPLY-000005.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 175\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.83it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.72it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.72it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.68it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:04,  4.64it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.66it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.66it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.68it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.66it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.67it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.66it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.67it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.67it/s]\u001b[A\n",
            " 50% 14/28 [00:02<00:02,  4.67it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.68it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.68it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.67it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.66it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.66it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.66it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.67it/s]\u001b[A\n",
            "100% 28/28 [00:05<00:00,  4.67it/s]\n",
            "epoch 6/10\n",
            "steps:  60% 210/350 [03:15<02:10,  1.08it/s, loss=0.16] saving checkpoint: /content/LoRA/output/REPLY-000006.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 210\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.60it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.62it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.53it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.55it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  4.57it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.59it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.61it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.61it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.59it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.61it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.57it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.49it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.50it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.52it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.56it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.56it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.56it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.55it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.56it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.58it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.59it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.60it/s]\u001b[A\n",
            " 82% 23/28 [00:05<00:01,  4.61it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.54it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.57it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.60it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.60it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.57it/s]\n",
            "epoch 7/10\n",
            "steps:  70% 245/350 [03:49<01:38,  1.07it/s, loss=0.123]saving checkpoint: /content/LoRA/output/REPLY-000007.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 245\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.82it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.74it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.71it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.66it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:04,  4.64it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.66it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.67it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.67it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.63it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.64it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:05,  3.29it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:04,  3.32it/s]\u001b[A\n",
            " 46% 13/28 [00:03<00:04,  3.43it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  3.75it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:03,  3.97it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.14it/s]\u001b[A\n",
            " 61% 17/28 [00:04<00:02,  4.23it/s]\u001b[A\n",
            " 64% 18/28 [00:04<00:02,  4.23it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:02,  4.15it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:02,  3.42it/s]\u001b[A\n",
            " 75% 21/28 [00:05<00:02,  3.24it/s]\u001b[A\n",
            " 79% 22/28 [00:05<00:01,  3.56it/s]\u001b[A\n",
            " 82% 23/28 [00:05<00:01,  3.83it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
            " 89% 25/28 [00:06<00:00,  4.16it/s]\u001b[A\n",
            " 93% 26/28 [00:06<00:00,  4.30it/s]\u001b[A\n",
            " 96% 27/28 [00:06<00:00,  4.37it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.09it/s]\n",
            "epoch 8/10\n",
            "steps:  80% 280/350 [04:25<01:06,  1.05it/s, loss=0.125]saving checkpoint: /content/LoRA/output/REPLY-000008.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 280\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.72it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.45it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.60it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.58it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  4.47it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.54it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.58it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.61it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.61it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.63it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.64it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.65it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.64it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.63it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.64it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.65it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.66it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.63it/s]\n",
            "epoch 9/10\n",
            "steps:  90% 315/350 [04:58<00:33,  1.06it/s, loss=0.184]saving checkpoint: /content/LoRA/output/REPLY-000009.safetensors\n",
            "generating sample images at step / サンプル画像生成 ステップ: 315\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.69it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.67it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.66it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.62it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  4.59it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.60it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.62it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.63it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.62it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.64it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.66it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.65it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.65it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.63it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.63it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.65it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.66it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.65it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.64it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.66it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.64it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.65it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.64it/s]\n",
            "epoch 10/10\n",
            "steps: 100% 350/350 [05:30<00:00,  1.06it/s, loss=0.142]generating sample images at step / サンプル画像生成 ステップ: 350\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:05,  4.83it/s]\u001b[A\n",
            "  7% 2/28 [00:00<00:05,  4.66it/s]\u001b[A\n",
            " 11% 3/28 [00:00<00:05,  4.31it/s]\u001b[A\n",
            " 14% 4/28 [00:00<00:05,  4.40it/s]\u001b[A\n",
            " 18% 5/28 [00:01<00:05,  4.49it/s]\u001b[A\n",
            " 21% 6/28 [00:01<00:04,  4.54it/s]\u001b[A\n",
            " 25% 7/28 [00:01<00:04,  4.57it/s]\u001b[A\n",
            " 29% 8/28 [00:01<00:04,  4.60it/s]\u001b[A\n",
            " 32% 9/28 [00:01<00:04,  4.58it/s]\u001b[A\n",
            " 36% 10/28 [00:02<00:03,  4.59it/s]\u001b[A\n",
            " 39% 11/28 [00:02<00:03,  4.61it/s]\u001b[A\n",
            " 43% 12/28 [00:02<00:03,  4.63it/s]\u001b[A\n",
            " 46% 13/28 [00:02<00:03,  4.63it/s]\u001b[A\n",
            " 50% 14/28 [00:03<00:03,  4.63it/s]\u001b[A\n",
            " 54% 15/28 [00:03<00:02,  4.63it/s]\u001b[A\n",
            " 57% 16/28 [00:03<00:02,  4.64it/s]\u001b[A\n",
            " 61% 17/28 [00:03<00:02,  4.65it/s]\u001b[A\n",
            " 64% 18/28 [00:03<00:02,  4.64it/s]\u001b[A\n",
            " 68% 19/28 [00:04<00:01,  4.63it/s]\u001b[A\n",
            " 71% 20/28 [00:04<00:01,  4.64it/s]\u001b[A\n",
            " 75% 21/28 [00:04<00:01,  4.64it/s]\u001b[A\n",
            " 79% 22/28 [00:04<00:01,  4.64it/s]\u001b[A\n",
            " 82% 23/28 [00:04<00:01,  4.64it/s]\u001b[A\n",
            " 86% 24/28 [00:05<00:00,  4.64it/s]\u001b[A\n",
            " 89% 25/28 [00:05<00:00,  4.63it/s]\u001b[A\n",
            " 93% 26/28 [00:05<00:00,  4.64it/s]\u001b[A\n",
            " 96% 27/28 [00:05<00:00,  4.64it/s]\u001b[A\n",
            "100% 28/28 [00:06<00:00,  4.61it/s]\n",
            "save trained model to /content/LoRA/output/REPLY.safetensors\n",
            "model saved.\n",
            "steps: 100% 350/350 [05:38<00:00,  1.04it/s, loss=0.142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6. Launch Portable Web UI\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import time\n",
        "import json\n",
        "from google.colab import drive\n",
        "from datetime import timedelta\n",
        "from subprocess import getoutput\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.chdir(root_dir)\n",
        "webui_dir = os.path.join(root_dir, \"stable-diffusion-webui\")\n",
        "tmp_dir = os.path.join(root_dir, \"tmp\")\n",
        "patches_dir = os.path.join(root_dir, \"patches\")\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "extensions_dir = os.path.join(webui_dir, \"extensions\")\n",
        "\n",
        "webui_models_dir = os.path.join(webui_dir, \"models/Stable-diffusion\")\n",
        "webui_lora_dir = os.path.join(extensions_dir, \"sd-webui-additional-networks/models/lora\")\n",
        "webui_vaes_dir = os.path.join(webui_dir, \"models/VAE\")\n",
        "webui_controlnet_dir = os.path.join(webui_dir, \"models/ControlNet\")\n",
        "\n",
        "A100 = \"https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15+e163309.d20230103.ColabProA100-cp38-cp38-linux_x86_64.whl\"\n",
        "\n",
        "enable_new_ui = True #@param {type:\"boolean\"}\n",
        "# 3cd625854f9dc71235a432703ba82abfc5d1a3fc\n",
        "stable_commit = \"\" \n",
        "update_extensions = True\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "package_url = [\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui.tar.lz4\",\n",
        "               \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-deps.tar.lz4\",\n",
        "               \"https://huggingface.co/Linaqruf/fast-repo/resolve/main/webui-cache.tar.lz4\"]\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  with capture.capture_output() as cap:\n",
        "    !wget -q --show-progress {url}\n",
        "    with zipfile.ZipFile(name, 'r') as deps:\n",
        "      deps.extractall(dst)\n",
        "    !dpkg -i {dst}/*\n",
        "    os.remove(name)\n",
        "    shutil.rmtree(dst)\n",
        "    del cap \n",
        "    \n",
        "def pre_download():\n",
        "  for package in tqdm(package_url, desc='\u001b[1;32mUnpacking WebUI'):\n",
        "    with capture.capture_output() as cap:\n",
        "      package_name = os.path.basename(package)\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {root_dir} -o {package_name} {package}\n",
        "      if package_name == \"webui-deps.tar.lz4\":\n",
        "        !tar -xI lz4 -f {package_name} --overwrite-dir --directory=/usr/local/lib/python3.9/dist-packages/\n",
        "      else:\n",
        "        !tar -xI lz4 -f {package_name} --directory=/\n",
        "      os.remove(os.path.basename(package_name))\n",
        "      del cap\n",
        "      \n",
        "  if os.path.exists(\"/usr/local/lib/python3.9/dist-packages/ffmpy-0.3.0.dist-info\"):\n",
        "    shutil.rmtree(\"/usr/local/lib/python3.9/dist-packages/ffmpy-0.3.0.dist-info\")\n",
        "\n",
        "  s = getoutput('nvidia-smi')\n",
        "  with capture.capture_output() as cap:\n",
        "    if not 'T4' in s:\n",
        "      !pip uninstall -y xformers\n",
        "      !pip install -q --pre xformers\n",
        "      !pip install -q --pre triton\n",
        "    del cap\n",
        "\n",
        "start_install = int(time.time())\n",
        "print(\"\u001b[1;32mInstalling...\\n\", end= \"\")\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "pre_download()\n",
        "\n",
        "if enable_new_ui:\n",
        "  print(\"\u001b[1;32mUsing new UI/UX from @Anapnoe...\")\n",
        "  with capture.capture_output() as cap:\n",
        "    os.chdir(webui_dir)\n",
        "    if os.path.exists(os.path.join(extensions_dir,\"sd-web-ui-quickcss/style.css\")):\n",
        "      os.remove(os.path.join(extensions_dir,\"sd-web-ui-quickcss/style.css\"))\n",
        "    !git remote set-url origin https://github.com/anapnoe/stable-diffusion-webui/\n",
        "    !git pull\n",
        "    if stable_commit:\n",
        "      !git reset --hard {stable_commit}\n",
        "    del cap\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  !wget https://raw.githubusercontent.com/ddPn08/automatic1111-colab/main/patches/stablediffusion-lowram.patch -P {patches_dir}  -c\n",
        "  os.chdir(os.path.join(webui_dir, \"repositories/stable-diffusion-stability-ai\"))\n",
        "  !git apply {patches_dir}/stablediffusion-lowram.patch\n",
        "\n",
        "  !sed -i \"s@os.path.splitext(checkpoint_.*@os.path.splitext(checkpoint_file); map_location='cuda'@\" /content/stable-diffusion-webui/modules/sd_models.py\n",
        "  !sed -i 's@ui.create_ui().*@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1)@' /content/stable-diffusion-webui/webui.py\n",
        "  !sed -i \"s@'cpu'@'cuda'@\" /content/stable-diffusion-webui/modules/extras.py\n",
        "  del cap\n",
        "  \n",
        "with open(os.path.join(webui_dir, \"config.json\"), \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data[\"outdir_txt2img_samples\"] = os.path.join(tmp_dir, \"outputs/txt2img-images\")\n",
        "data[\"outdir_img2img_samples\"] = os.path.join(tmp_dir, \"outputs/img2img-images\")\n",
        "data[\"outdir_extras_samples\"] = os.path.join(tmp_dir, \"outputs/extras-images\")\n",
        "data[\"outdir_txt2img_grids\"] = os.path.join(tmp_dir, \"outputs/txt2img-grids\")\n",
        "data[\"outdir_img2img_grids\"] = os.path.join(tmp_dir, \"outputs/img2img-grids\")\n",
        "data[\"outdir_save\"] = os.path.join(tmp_dir, \"log/images\")\n",
        "\n",
        "with open(os.path.join(webui_dir, \"config.json\"), \"w\") as f:\n",
        "  json.dump(data, f, indent=4)\n",
        "  \n",
        "install_time = timedelta(seconds=time.time()-start_install)\n",
        "print(\"\u001b[1;32mFinished unpacking. Took\",\"%02d:%02d:%02d \\n\" % (install_time.seconds / 3600, (install_time.seconds / 60) % 60, install_time.seconds % 60), end='', flush=True)\n",
        "\n",
        "update = int(time.time())\n",
        "if update_extensions:\n",
        "  extensions_updated = []\n",
        "  with tqdm(total=len(os.listdir(extensions_dir)), desc=\"\u001b[1;32mUpdating extensions\",  mininterval=0) as pbar:\n",
        "    for dir in os.listdir(extensions_dir):\n",
        "      if os.path.isdir(os.path.join(extensions_dir, dir)):\n",
        "        os.chdir(os.path.join(extensions_dir, dir))\n",
        "        with capture.capture_output() as cap:\n",
        "          !git fetch origin\n",
        "          !git pull\n",
        "          \n",
        "        output = cap.stdout.strip()\n",
        "        if \"Already up to date.\" not in output:\n",
        "          extensions_updated.append(dir)\n",
        "        pbar.update(1)\n",
        "  print(\"\\n\")\n",
        "  for ext in extensions_updated:\n",
        "    print(f\"\u001b[1;32m{ext} updated to new version\")\n",
        "  update_time = timedelta(seconds=time.time()-update)\n",
        "  print(\"\\n\u001b[1;32mAll extensions are up to date. Took\",\"%02d:%02d:%02d\" % (update_time.seconds / 3600, (update_time.seconds / 60) % 60, update_time.seconds % 60), end='', flush=True)\n",
        "\n",
        "print('\\n\u001b[1;32mAll is done! Go to the next step.')\n",
        "\n",
        "#@markdown > Get <b>your</b> token for ngrok [here](https://dashboard.ngrok.com/get-started/your-authtoken) \n",
        "ngrok_token = \"2MuTSVJTT6hNWQRW9GfDSaHlWj0_5ddcf9SCYdJAidrX2eVWn\" #@param {type: 'string'}\n",
        "ngrok_region = \"eu\" #@param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  for file in os.listdir(output_dir):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    if file_path.endswith((\".safetensors\", \".pt\", \".ckpt\")):\n",
        "      !ln \"{file_path}\" {webui_lora_dir}\n",
        "\n",
        "  for file in os.listdir(pretrained_model):\n",
        "    file_path = os.path.join(pretrained_model, file)\n",
        "    if file_path.endswith((\".safetensors\", \".ckpt\")):\n",
        "      !ln \"{file_path}\" {webui_models_dir}\n",
        "\n",
        "  for file in os.listdir(vae_dir):\n",
        "    file_path = os.path.join(vae_dir, file)\n",
        "    if file_path.endswith(\".vae.pt\"):\n",
        "      !ln \"{file_path}\" {webui_vaes_dir}\n",
        "\n",
        "  for file in os.listdir(controlnet_dir):\n",
        "    file_path = os.path.join(controlnet_dir, file)\n",
        "    !ln \"{file_path}\" {webui_controlnet_dir}\n",
        "\n",
        "  del cap\n",
        "\n",
        "os.chdir(webui_dir)\n",
        "\n",
        "print(\"\u001b[1;32m\")\n",
        "\n",
        "!python launch.py \\\n",
        "  --enable-insecure-extension-access \\\n",
        "  --disable-safe-unpickle \\\n",
        "  --lora-dir {webui_lora_dir} \\\n",
        "  --ckpt-dir {webui_models_dir} \\\n",
        "  --vae-dir {webui_vaes_dir} \\\n",
        "  {\"--share\" if not ngrok_token else \"\"} \\\n",
        "  --no-half-vae \\\n",
        "  --lowram \\\n",
        "  --no-hashing \\\n",
        "  --disable-console-progressbars \\\n",
        "  {\"--ngrok \" + ngrok_token if ngrok_token else \"\"} \\\n",
        "  {\"--ngrok-region \" + ngrok_region if ngrok_token else \"\"} \\\n",
        "  --xformers \\\n",
        "  --opt-sub-quad-attention \\\n",
        "  --opt-channelslast \\\n",
        "  --theme dark"
      ],
      "metadata": {
        "id": "dglQfu3A0oaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36f3007-d72d-4fbb-903e-1e7372b4c2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32mInstalling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;32mUnpacking WebUI: 100%|██████████| 3/3 [00:24<00:00,  8.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32mUsing new UI/UX from @Anapnoe...\n",
            "\u001b[1;32mFinished unpacking. Took 00:00:30 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;32mUpdating extensions:  94%|█████████▍| 15/16 [00:07<00:00,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1;32mstable-diffusion-webui-images-browser updated to new version\n",
            "\u001b[1;32msd-webui-llul updated to new version\n",
            "\u001b[1;32msd-webui-controlnet updated to new version\n",
            "\u001b[1;32msd-webui-additional-networks updated to new version\n",
            "\u001b[1;32ma1111-sd-webui-locon updated to new version\n",
            "\n",
            "\u001b[1;32mAll extensions are up to date. Took 00:00:07"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;32mAll is done! Go to the next step.\n",
            "\u001b[1;32m\n",
            "Python 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "Commit hash: 0cc0ee1bcb4c24a8c9715f66cede06601bfc00c8\n",
            "Installing requirements for Web UI\n",
            "\n",
            "\n",
            "\n",
            "Launching Web UI with arguments: --enable-insecure-extension-access --disable-safe-unpickle --lora-dir /content/stable-diffusion-webui/extensions/sd-webui-additional-networks/models/lora --ckpt-dir /content/stable-diffusion-webui/models/Stable-diffusion --vae-dir /content/stable-diffusion-webui/models/VAE --no-half-vae --lowram --no-hashing --disable-console-progressbars --ngrok 2MuTSVJTT6hNWQRW9GfDSaHlWj0_5ddcf9SCYdJAidrX2eVWn --ngrok-region eu --xformers --opt-sub-quad-attention --opt-channelslast --theme dark\n",
            "2023-03-12 17:35:22.786824: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-03-12 17:35:25.230223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-12 17:35:25.230488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-12 17:35:25.230524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "ngrok authtoken detected, trying to connect...\n",
            "ngrok connected to localhost:7860! URL: https://8415-34-83-143-85.eu.ngrok.io\n",
            "You can use this link after the launch is complete.\n",
            "LoCon Extension hijack addnet extension successfully\n",
            "LoCon Extension hijack built-in lora successfully\n",
            "[AddNet] Updating model hashes...\n",
            "100% 10/10 [00:00<00:00, 137.39it/s]\n",
            "[AddNet] Updating model hashes...\n",
            "100% 10/10 [00:00<00:00, 9504.43it/s]\n",
            "Checkpoint anything_v3_3.safetensors not found; loading fallback control_sd15_depth.ckpt\n",
            "Loading weights [None] from /content/stable-diffusion-webui/models/Stable-diffusion/control_sd15_depth.ckpt\n",
            "Creating model from config: /content/stable-diffusion-webui/configs/v1-inference.yaml\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "Couldn't find VAE named anime.vae.pt; using None instead\n",
            "Applying xformers cross attention optimization.\n",
            "Textual inversion embeddings loaded(0): \n",
            "Model loaded in 40.0s (load weights from disk: 37.3s, create model: 2.5s).\n",
            "Batchlinks webui extension: (Optional) Use --gradio-queue args to enable logging & cancel button on this extension\n",
            "Image Browser: Database upgraded from version 5 to version 6\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n",
            "Loading weights [None] from /content/stable-diffusion-webui/models/Stable-diffusion/Anything-v4-5.ckpt\n",
            "Loading VAE weights specified in settings: /content/stable-diffusion-webui/models/VAE/stablediffusion.vae.pt\n",
            "Applying xformers cross attention optimization.\n",
            "Weights loaded in 30.2s (load weights from disk: 22.8s, apply weights to model: 4.4s, load VAE: 2.2s, move model to device: 0.7s).\n",
            "LoRA weight_unet: 1, weight_tenc: 1, model: REPLY(99c0d612754a)\n",
            "dimension: 32,\n",
            "alpha: 16.0,\n",
            "multiplier_unet: 1,\n",
            "multiplier_tenc: 1\n",
            "create LoCon for Text Encoder: 72 modules.\n",
            "create LoCon for U-Net: 278 modules.\n",
            "original forward/weights is backed up.\n",
            "enable LoCon for text encoder\n",
            "enable LoCon for U-Net\n",
            "shapes for 0 weights are converted.\n",
            "LoRA model REPLY(99c0d612754a) loaded: <All keys matched successfully>\n",
            "setting (or sd model) changed. new networks created.\n",
            "100% 20/20 [00:12<00:00,  1.66it/s]\n",
            "100% 20/20 [00:06<00:00,  3.05it/s]\n",
            "100% 20/20 [00:06<00:00,  3.02it/s]\n",
            "100% 20/20 [00:06<00:00,  3.06it/s]\n",
            "Loading model: control_sd15_depth [fef5e48e]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 7.1. Compressing model or dataset\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_module = \"zipfile\" #@param [\"zipfile\", \"shutil\", \"pyminizip\", \"zip\"]\n",
        "directory_to_zip = '/content/LoRA/output' #@param {type: \"string\"}\n",
        "output_filename = f'/content/{project_name}.zip' #@param {type: \"string\"}\n",
        "password = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if zip_module == \"zipfile\":\n",
        "    with zipfile.ZipFile(output_filename, 'w') as zip:\n",
        "        for directory_to_zip, dirs, files in os.walk(directory_to_zip):\n",
        "            for file in files:\n",
        "                zip.write(os.path.join(directory_to_zip, file))\n",
        "elif zip_module == \"shutil\":\n",
        "    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
        "elif zip_module == \"pyminizip\":\n",
        "    !pip install pyminizip\n",
        "    import pyminizip\n",
        "    for root, dirs, files in os.walk(directory_to_zip):\n",
        "        for file in files:\n",
        "            pyminizip.compress(os.path.join(root, file), \"\", os.path.join(\"*\",output_filename), password, 5)\n",
        "elif zip_module == \"zip\":\n",
        "    !zip -rv -q -j {output_filename} {directory_to_zip}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rLdEpPKTbI1I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}